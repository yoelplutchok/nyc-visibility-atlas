#!/usr/bin/env python3
"""
11_consequence_vulnerability.py

Define consequence layer and score neighborhoods by low-visibility vulnerability.

Pipeline Step: 11
Contract Reference: Section 11 - 11_consequence_vulnerability.py

This script:
1. Identifies neighborhoods with high consequence potential (high mortality/morbidity rates)
2. Identifies neighborhoods with low visibility across data sources
3. Computes vulnerability scores combining consequence and invisibility

Inputs:
    - data/processed/visibility/visibility_long.parquet
    - data/processed/matrix/visibility_matrix_pctrank.parquet
    - data/processed/typologies/typology_assignments.parquet

Outputs:
    - data/processed/vulnerability/vulnerability_scores.parquet
    - reports/tables/vulnerability_priority_list.md
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

import logging
from datetime import datetime, timezone

import numpy as np
import pandas as pd

from visibility_atlas.paths import paths, ensure_dir
from visibility_atlas.logging_utils import (
    get_logger, log_step_start, log_step_end,
    log_qa_check, log_output_written, get_run_id
)
from visibility_atlas.io_utils import (
    atomic_write_parquet, read_parquet, atomic_write_text
)
from visibility_atlas.hashing import write_metadata_sidecar


SCRIPT_NAME = "11_consequence_vulnerability"


def compute_consequence_score(
    visibility_long: pd.DataFrame,
    logger: logging.Logger
) -> pd.DataFrame:
    """
    Compute consequence score based on available health outcomes.
    
    For now, uses SPARCS hospitalization rates as proxy for health burden.
    Higher SPARCS visibility (more hospitalizations) = higher consequence.
    """
    log_step_start(logger, "compute_consequence_score")
    
    # Get SPARCS visibility as consequence proxy
    sparcs = visibility_long[visibility_long['source_id'] == 'sparcs'][
        ['geo_id', 'visibility']
    ].copy()
    sparcs = sparcs.rename(columns={'visibility': 'hosp_rate'})
    
    # Get vital (mortality) as additional consequence indicator
    vital = visibility_long[visibility_long['source_id'] == 'vital'][
        ['geo_id', 'visibility']
    ].copy()
    vital = vital.rename(columns={'visibility': 'mortality_rate'})
    
    # Merge
    consequence = sparcs.merge(vital, on='geo_id', how='outer')
    
    # Normalize to percentiles (higher = worse outcome = higher consequence)
    if 'hosp_rate' in consequence.columns and consequence['hosp_rate'].notna().sum() > 0:
        consequence['hosp_pctile'] = consequence['hosp_rate'].rank(pct=True) * 100
    else:
        consequence['hosp_pctile'] = 50
    
    if 'mortality_rate' in consequence.columns and consequence['mortality_rate'].notna().sum() > 0:
        consequence['mortality_pctile'] = consequence['mortality_rate'].rank(pct=True) * 100
    else:
        consequence['mortality_pctile'] = 50
    
    # Combined consequence score (average of available indicators)
    pctile_cols = [c for c in ['hosp_pctile', 'mortality_pctile'] 
                   if c in consequence.columns]
    consequence['consequence_score'] = consequence[pctile_cols].mean(axis=1)
    
    logger.info(f"Computed consequence scores for {len(consequence)} neighborhoods")
    logger.info(f"Consequence score range: {consequence['consequence_score'].min():.1f} - {consequence['consequence_score'].max():.1f}")
    
    log_step_end(logger, "compute_consequence_score", n_neighborhoods=len(consequence))
    return consequence


def compute_invisibility_score(
    visibility_long: pd.DataFrame,
    pctrank_matrix: pd.DataFrame,
    logger: logging.Logger
) -> pd.DataFrame:
    """
    Compute invisibility score - how poorly a neighborhood is captured by surveys.
    
    Lower CHS visibility = higher invisibility.
    """
    log_step_start(logger, "compute_invisibility_score")
    
    # CHS visibility is survey capture
    chs = visibility_long[visibility_long['source_id'] == 'chs'][
        ['geo_id', 'visibility']
    ].copy()
    chs = chs.rename(columns={'visibility': 'survey_visibility'})
    
    # Compute invisibility as inverse of survey visibility percentile
    chs['survey_pctile'] = chs['survey_visibility'].rank(pct=True) * 100
    chs['invisibility_score'] = 100 - chs['survey_pctile']
    
    # Get mean visibility across all sources from pctrank matrix
    source_cols = ['chs', 'sparcs', 'vital']
    source_cols = [c for c in source_cols if c in pctrank_matrix.columns]
    
    pctrank_subset = pctrank_matrix[['geo_id'] + source_cols].copy()
    pctrank_subset['mean_visibility_pctile'] = pctrank_subset[source_cols].mean(axis=1)
    pctrank_subset['multi_system_invisibility'] = 100 - pctrank_subset['mean_visibility_pctile']
    
    # Merge
    invisibility = chs.merge(
        pctrank_subset[['geo_id', 'mean_visibility_pctile', 'multi_system_invisibility']],
        on='geo_id', how='outer'
    )
    
    logger.info(f"Computed invisibility scores for {len(invisibility)} neighborhoods")
    
    log_step_end(logger, "compute_invisibility_score", n_neighborhoods=len(invisibility))
    return invisibility


def compute_vulnerability_scores(
    consequence: pd.DataFrame,
    invisibility: pd.DataFrame,
    logger: logging.Logger
) -> pd.DataFrame:
    """
    Combine consequence and invisibility into vulnerability scores.
    
    High vulnerability = high consequence + high invisibility
    """
    log_step_start(logger, "compute_vulnerability_scores")
    
    # Merge consequence and invisibility
    vuln = consequence.merge(invisibility, on='geo_id', how='outer')
    
    # Compute combined vulnerability score
    # Neighborhoods with high health burden AND low survey visibility are most vulnerable
    vuln['vulnerability_score'] = (
        vuln['consequence_score'].fillna(50) * 0.5 +
        vuln['invisibility_score'].fillna(50) * 0.5
    )
    
    # Alternative: multiplicative (only high if both are high)
    vuln['vulnerability_multiplicative'] = (
        (vuln['consequence_score'].fillna(50) / 100) *
        (vuln['invisibility_score'].fillna(50) / 100) * 100
    )
    
    # Categorize vulnerability
    vuln['vulnerability_category'] = pd.cut(
        vuln['vulnerability_score'],
        bins=[0, 33, 66, 100],
        labels=['low', 'medium', 'high']
    )
    
    # Rank
    vuln['vulnerability_rank'] = vuln['vulnerability_score'].rank(ascending=False)
    
    logger.info(f"Computed vulnerability for {len(vuln)} neighborhoods")
    logger.info(f"Vulnerability distribution: {vuln['vulnerability_category'].value_counts().to_dict()}")
    
    log_step_end(logger, "compute_vulnerability_scores", n_neighborhoods=len(vuln))
    return vuln


def create_priority_list_md(
    vulnerability: pd.DataFrame,
    output_path: Path,
    logger: logging.Logger
):
    """Create markdown priority list of high-vulnerability neighborhoods."""
    
    # Get top 20 most vulnerable
    top_vuln = vulnerability.nsmallest(20, 'vulnerability_rank')
    
    lines = [
        "# Visibility Vulnerability Priority List",
        "",
        "## Overview",
        "Neighborhoods ranked by combined **consequence** (health burden) and **invisibility** (survey underrepresentation).",
        "",
        "High vulnerability = neighborhoods with significant health issues that are poorly captured by surveys.",
        "",
        "## Top 20 Most Vulnerable Neighborhoods",
        "",
        "| Rank | NTA | Vulnerability Score | Consequence | Invisibility | Category |",
        "|------|-----|---------------------|-------------|--------------|----------|",
    ]
    
    for _, row in top_vuln.iterrows():
        lines.append(
            f"| {int(row['vulnerability_rank'])} | {row['geo_id']} | "
            f"{row['vulnerability_score']:.1f} | "
            f"{row.get('consequence_score', 0):.1f} | "
            f"{row.get('invisibility_score', 0):.1f} | "
            f"{row.get('vulnerability_category', 'N/A')} |"
        )
    
    lines.extend([
        "",
        "## Interpretation",
        "",
        "- **Consequence Score (0-100):** Based on hospitalization and mortality rates. Higher = worse health outcomes.",
        "- **Invisibility Score (0-100):** Based on survey underrepresentation. Higher = less visible to surveys.",
        "- **Vulnerability Score:** Average of consequence and invisibility.",
        "",
        "## Policy Implications",
        "",
        "Neighborhoods with high vulnerability scores may:",
        "- Be underrepresented in health surveys",
        "- Have health issues that are not captured in official statistics",
        "- Benefit from targeted outreach and data collection efforts",
    ])
    
    atomic_write_text(output_path, "\n".join(lines))
    logger.info(f"Wrote priority list to {output_path}")


def main():
    """Main entry point."""
    run_id = get_run_id()
    logger = get_logger(SCRIPT_NAME, run_id)
    
    logger.info("=" * 60)
    logger.info(f"Starting {SCRIPT_NAME}")
    logger.info(f"Run ID: {run_id}")
    logger.info("=" * 60)
    
    try:
        # Load data
        visibility = read_parquet(paths.processed_visibility / "visibility_long.parquet")
        pctrank = read_parquet(paths.processed_matrix / "visibility_matrix_pctrank.parquet")
        
        logger.info(f"Loaded visibility: {len(visibility)} rows")
        logger.info(f"Loaded pctrank matrix: {len(pctrank)} rows")
        
        # Compute scores
        consequence = compute_consequence_score(visibility, logger)
        invisibility = compute_invisibility_score(visibility, pctrank, logger)
        vulnerability = compute_vulnerability_scores(consequence, invisibility, logger)
        
        # Write outputs
        output_dir = ensure_dir(paths.processed_vulnerability)
        
        vuln_path = output_dir / "vulnerability_scores.parquet"
        atomic_write_parquet(vuln_path, vulnerability)
        log_output_written(logger, vuln_path, row_count=len(vulnerability))
        
        # Write priority list
        reports_dir = ensure_dir(paths.reports / "tables")
        priority_path = reports_dir / "vulnerability_priority_list.md"
        create_priority_list_md(vulnerability, priority_path, logger)
        
        # Write metadata
        write_metadata_sidecar(
            vuln_path,
            run_id,
            parameters={
                "n_neighborhoods": len(vulnerability),
                "vulnerability_distribution": vulnerability['vulnerability_category'].value_counts().to_dict(),
            },
            row_count=len(vulnerability),
        )
        
        # QA checks
        high_vuln_count = (vulnerability['vulnerability_category'] == 'high').sum()
        log_qa_check(logger, "high_vulnerability_count", True,
                    f"{high_vuln_count} neighborhoods classified as high vulnerability")
        
        # Summary
        logger.info("=" * 60)
        logger.info(f"âœ… {SCRIPT_NAME} completed successfully")
        logger.info(f"   Neighborhoods analyzed: {len(vulnerability)}")
        logger.info(f"   Vulnerability distribution: {vulnerability['vulnerability_category'].value_counts().to_dict()}")
        logger.info(f"   Top 5 most vulnerable: {vulnerability.nsmallest(5, 'vulnerability_rank')['geo_id'].tolist()}")
        logger.info("=" * 60)
        
        return 0
        
    except Exception as e:
        logger.exception(f"Pipeline failed: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())

